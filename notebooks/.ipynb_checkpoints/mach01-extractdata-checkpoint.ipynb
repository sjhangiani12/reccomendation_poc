{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharan/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/Users/sharan/.matplotlib/matplotlibrc\", line #39\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking all folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [x[0] for x in os.walk(str(os.path.abspath('..') + '/data/stories/'))]\n",
    "folders[0] = folders[0][:len(folders[0])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the file names and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "c = False\n",
    "for i in folders:\n",
    "    file = open(i+\"/index.html\", 'r')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
    "    file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
    "    if c == False:\n",
    "        file_name = file_name[2:]\n",
    "        c = True\n",
    "    for j in range(len(file_name)):\n",
    "        dataset.append((str(i) +\"/\"+ str(file_name[j]), file_title[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc(id):\n",
    "    print(dataset[id])\n",
    "    file = open(dataset[id][0], 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "processed_title = []\n",
    "texts = []\n",
    "titles = []\n",
    "\n",
    "for i in dataset[:len(dataset)]:\n",
    "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    \n",
    "    titles.append(i[1])\n",
    "    texts.append(text)\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = texts\n",
    "df['indexed_texts'] = processed_text\n",
    "df['indexed_titles'] = processed_title\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating DF for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_freq = {}\n",
    "\n",
    "for index in range(len(df)):\n",
    "    tokens = df.indexed_texts[index]\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            doc_freq[word].add(index)\n",
    "        except:\n",
    "            doc_freq[word] = {index}\n",
    "\n",
    "    tokens = df.indexed_titles[index]\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            doc_freq[word].add(index)\n",
    "        except:\n",
    "            doc_freq[word] = {index}\n",
    "for index in doc_freq:\n",
    "    doc_freq[index] = len(doc_freq[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_words = len(doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_words = [x for x in doc_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sharewar', 'trial', 'project', 'freewar', 'need', 'support', 'continu', 'one', 'hundr', 'west', 'fifti', 'three', 'north', 'jim', 'prentic', 'copyright', 'thousand', 'nine', 'nineti', 'brandon']\n"
     ]
    }
   ],
   "source": [
    "print(all_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_freq(word):\n",
    "    try:\n",
    "        return DF[word]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating TF-IDF for body, we will consider this as the actual tf-idf as we will add the title weight to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = 0\n",
    "tf_idf_text = {}\n",
    "for index in range(len(df)):\n",
    "    tokens = df.indexed_texts[index]\n",
    "    counter = Counter(tokens + df.indexed_titles[index])\n",
    "    words_count = len(tokens + df.indexed_titles[index])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        #apply tf-idf equation\n",
    "        total_frequency = counter[token]/words_count\n",
    "        doc_freq = get_doc_freq(token)\n",
    "        inverted_document_frequency = np.log((len(df)+1)/(doc_freq+1))\n",
    "        tf_idf_text[doc, token] = total_frequency*inverted_document_frequency\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating TF-IDF for Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "tf_idf_title = {}\n",
    "for index in range(len(df)):\n",
    "    tokens = df.indexed_titles[index]\n",
    "    counter = Counter(tokens + df.indexed_texts[index])\n",
    "    words_count = len(tokens + df.indexed_texts[index])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        #apply tf-idf equation\n",
    "        total_frequency = counter[token]/words_count\n",
    "        doc_freq = get_doc_freq(token)\n",
    "        inverted_document_frequency = np.log((len(df)+1)/(doc_freq+1))\n",
    "        tf_idf_title[doc, token] = total_frequency*inverted_document_frequency\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(10, 'Horror')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-3e224bcc1e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_idf_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Horror\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: (10, 'Horror')"
     ]
    }
   ],
   "source": [
    "tf_idf_text[(10,\"Horror\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344378"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_title[(0,\"go\")]\n",
    "len(tf_idf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the TF-IDF according to weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a 0.3 depreciating weight to textual tf-idf values\n",
    "alpha = 0.3\n",
    "for i in tf_idf_text:\n",
    "    tf_idf_text[i] *= alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tf_idf_title:\n",
    "    tf_idf_text[i] = tf_idf_title[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344378"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_idf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Matching Score Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: Horror\n",
      "\n",
      "['horror']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(11, 'horror')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-13a2ccc6614a>\u001b[0m in \u001b[0;36mmatching_score\u001b[0;34m(k, query)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mquery_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 11",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-13a2ccc6614a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmatching_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Horror\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-111-13a2ccc6614a>\u001b[0m in \u001b[0;36mmatching_score\u001b[0;34m(k, query)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mquery_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mquery_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mquery_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (11, 'horror')"
     ]
    }
   ],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf_text:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    print(l)\n",
    "    \n",
    "\n",
    "matching_score(10, \"Horror\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Users/sharan/aarhus_itk/Information-Retrieval/2. TF-IDF Ranking - Cosine Similarity, Matching Score/stories/bluebrd.txt', 'Bluebeard')\n",
      "BLUEBEARD\n",
      "\n",
      "   Once upon a time... in the fair land of France, there lived a very powerful \n",
      "lord, the owner of estates, farms and a great splendid castle, and his name was\n",
      "Bluebeard. This wasn't his real name, it was a nickname, due to the fact he had\n",
      "a long shaggy black beard with glints of blue in it. He was very handsome and\n",
      "charming, but, if the truth be told, there was something about him that made \n",
      "you feel respect, and a little uneasy...\n",
      "   Bluebeard often went away to war, and when he did, he left his wife in \n",
      "charge of the castle... He had had lots of wives, all young, pretty and noble.\n",
      "As bad luck would have it, one after the other, they had all died, and so the\n",
      "noble lord was forever getting married again.\n",
      "   \"Sire,\" someone would ask now and again, \"what did your wives die of?\"\n",
      "   \"Hah, my friend,\" Bluebeard would reply, \"one died of smallpox, one of a \n",
      "hidden sickness, another of a high fever, another of a terrible infection...\n",
      "Ah, I'm very unlucky, and they're unlucky too! They're all buried in the castle\n",
      "chapel,\" he added. Nobody found anything strange about that. Nor did the sweet\n",
      "and beautiful young girl that Bluebeard took as a wife think it strange either.\n",
      "She went to the castle accompanied by her sister Anna, who said:\n",
      "   \"Oh, aren't you lucky marrying a lord like Bluebeard?\"\n",
      "   \"He really is very nice... and when you're close, his beard doesn't look as\n",
      "blue as folk say!\" said the bride, and the two sisters giggled delightedly.\n",
      "Poor souls! They had no idea what lay in store for them!...\n",
      "   A month or so later, Bluebeard had the carriage brought round and said to\n",
      "his wife, \"Darling, I must leave you for a few weeks. But keep cheerful during\n",
      "that time, invite whoever you like and look after the castle. Here,\" he added,\n",
      "handing his bride a bunch of keys, \"you'll need these, the keys of the safe,\n",
      "the armoury and the library keys, and this one, which opens all the room doors.\n",
      "Now, this little key here,\" and he pointed to a key that was much smalle than\n",
      "the others, \"opens the little room at the end of the great ground floor \n",
      "corridor. Take your friends were you want, open any door you like, but not this\n",
      "one! Is that quite clear?\" repeated Bluebeard. \"Not this one! Nobody at all is\n",
      "allowed to enter that little room. And if you ever did go into it, I would go \n",
      "into such a terrible rage that it's better that you don't!\"\n",
      "   \"Don't worry, husband,\" said Bluebeard's wife as she took the keys, \"I'll do\n",
      "as you say.\" After giving her a hug, Bluebeard got into his carriage, whipped\n",
      "up the horses and off he went.\n",
      "   The days went by. The young girl invited her friends to the castle and\n",
      "showed them round all the rooms except the one at the end of the corridor.\n",
      "   \"Why shouldn't I see inside the little room? Why? Why is it forbidden?\" \n",
      "Well, she thought about it so much that she ended up bursting with curiosity,\n",
      "until one day she opened the door and walked into the little room... Of all\n",
      "ghastly horrors! Inside, hanging on the walls were the bodies of Bluebeard's\n",
      "wivws: he had strangled them all with his own hands!\n",
      "   Terror stricken, the girl ran out of the room, but the bunch of keys slipped\n",
      "from her grasp. She picked them up without a glance and hurried to her own\n",
      "room, her heart thumping wildly in her chest. Horrors! She was living ina \n",
      "castle of the dead! So that is what had ahppened to Bluebeard's other wives!\n",
      "   The girl summoned up her courage and she noticed that one of the keys - the\n",
      "very key to the little room - was stained with blood.\n",
      "   \"I must wipe it clean, before my husband comes back!\" she said to herself.\n",
      "But try as she would, the blood stain wouldn't wash away. She washed, she\n",
      "scrubbed and she rinsed it; all in vain, for the key was still red. That very\n",
      "evening, Bluebeard came home. Just imagine the state his poor wife was in!\n",
      "   Bluebeard did not ask his wife for the keys that same evening, but he\n",
      "remarked:\n",
      "   \"You look a little upset, darling. Has anything nasty happened?\"\n",
      "   \"Oh, no! No!\"\n",
      "   \"Are you sorry I came back so soon?\"\n",
      "   \"Oh, no! I'm delighted!\" But that night, the bride didn't sleep a wink. Next\n",
      "day, Bluebeard said:\n",
      "   \"Darling, give me back the keys,\" and his wife hurriedly did so. Bluebeard\n",
      "remarked: \"There's one missing, the key to the little room!\"\n",
      "   \"Is there?\" said the young girl shaking,\n",
      "   \"I must have left it in my room!\"\n",
      "   \"All right, go and get it.\" But when Bluebeard's wife put the key into his\n",
      "hand, Bluebeard turned white and in a deep hoarse voice demanded:\n",
      "   \"Why is this key stained with blood?\"\n",
      "   \"I don't know...\" stammered his wife.\n",
      "   \"You know very well!\" he retorted. \"You went into the little room, didn't \n",
      "you? Well, you'll go back again, this time for good, along with the other \n",
      "ladies in there. You must die!\"\n",
      "   \"Oh no! I pray you!\"\n",
      "   \"You must die!\" he repeated. Just then, there was a knock at the door and\n",
      "Anna, Bluebeard's wife's sister, entered the castle.\n",
      "   \"Good morning,\" she said, \"you seem rather pale.\"\n",
      "   \"Not at all, we're quite well,\" replied Bluebeard. His wife whispered in his\n",
      "ear:\n",
      "   \"Please, please give me ten minutes to live!\" Bluebeard replied:\n",
      "   \"Not more than ten!\" The girl ran to her sister Anna whohad gone up to one\n",
      "of the towers and asked her,\n",
      "   \"Anna, do you see ou brothers coming? They promised they would come and see\n",
      "me today!\" But Anna replied\"\n",
      "   \"No, I don't see anyone. What's wrong? You look agitated.\"\n",
      "   \"Anna, please,\" said the shaken girl, \"look again! Are you sure you can't\n",
      "see someone?\"\n",
      "   \"No,\" said her sister, \"only one or two peasants.\" Just then the voice of\n",
      "Bluebeard boomed up to them:\n",
      "   \"Wife, your time is up! Come here!\"\n",
      "   \"I'm coming!\" she called, but then said to her sister: \"Oh Anna, aren't our\n",
      "brothers coming?...\"\n",
      "   \"No,\" replied Anna. Again Bluebeard shouted up.\n",
      "   \"Come down at once! Or I'll come up!\" Trembling like a leaf, his wife went\n",
      "downstairs. Bluebeard was clutching a big knife and he grabbed his bride by the\n",
      "hair...\n",
      "   \"Sister, I can see two horsemen coming!\" called out Anna from the tower that\n",
      "very moment. Bluebeard made a horrible face:\n",
      "   \"They too will die!\" His wife knelt to implore:\n",
      "   \"Please, please don't kill me. I'll never tell anyone what I saw! I'll never\n",
      "say a word!\"\n",
      "   \"Yes, you'll never say a word for eternity!\" snarled Bluebeard, raising his \n",
      "knife. The poor girl screamed:\n",
      "   \"Have pity on me!\" But he fiercely replied:\n",
      "   \"No! You must die!\" He was about to bring the knife down on the girl's\n",
      "delicate neck, when two young men burst into the room: a dragoon and a \n",
      "musketeer. They were his wife's brothers.\n",
      "   Drawing their swords, they leapt towards Bluebeard, who tried to flee up\n",
      "some stairs, but was caught and killed. And that was the end of the sad story.\n",
      "Bluebeard's poor wives were given a Christian burial, the castle was completely\n",
      "renovated and the young widow, some time later, married a good and honest young\n",
      "man, who helped her to forget the terrible adventure. And that young lady\n",
      "completely lost all her sense of curiosity...\n"
     ]
    }
   ],
   "source": [
    "print_doc(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Cosine Similarity Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorising tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "\n",
      "Query: 51\n",
      "\n",
      "['fifti', 'one']\n",
      "\n",
      "[  0 101 167 211 127 307  44 154  87 438]\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(out)\n",
    "\n",
    "#     for i in out:\n",
    "#         print(i, dataset[i][0])\n",
    "\n",
    "Q = cosine_similarity(10, \"Adventure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Users/sharan/aarhus_itk/Information-Retrieval/2. TF-IDF Ranking - Cosine Similarity, Matching Score/stories/bern', 'The Adventures of Bert and Bernece, by  Francis U. Kaltenbaugh')\n",
      "THE ADVENTURES OF BERT AND BERNECE\n",
      "  by Francis U. Kaltenbaugh\n",
      "\n",
      "  In mid-town, the sun's brazen harshness was reinforced, as\n",
      "it glared from a glass and ivory colored office building towering \n",
      "towards the heavens, stiff and erect in stature; symbolism oozed \n",
      "from its solar-heated shaft, as an unnoticed conversation unfolded \n",
      "ensconced near the tip of this man-made erection of glass and steel.\n",
      "  \n",
      "  \"Stop squirming. You'll die for what you did,\" Bert threatened.\n",
      "  \n",
      "  \"You'll never get away with this,\" I lied. \"There are others, who \n",
      "know I came here for you.\"\n",
      "  \n",
      "  \"You stole my woman; you're gonna pay,\" Bert accused.\n",
      "  \n",
      "  \"What woman? I don't have a woman -- not me. I'm to enter seminary \n",
      "next month. I'm celibate,\" I babbled.\n",
      "  \n",
      "  \"Sell a bit!  What the hell ... a polite way to say pimp or \n",
      "whoremaster?\" he implicated. His eyes were bulging -- matching the \n",
      "bulge in my genes.\n",
      "  \n",
      "  The situation couldn't get worse.  \n",
      "        \n",
      "  On the roof of his office building, near the ledge, my hands bound -- \n",
      "there was little hope. Bert had gone over the edge and wanted to see \n",
      "me there -- too. \n",
      "  \n",
      "  \"I can help get your woman back.\" I entreated.\n",
      "  \n",
      "  \"Ha. You took her from me!\" he inculpated.\n",
      "  \n",
      "  \"Bert, I couldn't take her from you. I'm your friend. I could never \n",
      "harm you. It'd be against my vows,\" I acquiesced.\n",
      "  \n",
      "  \"To your death,\" he sentenced.\n",
      "  \n",
      "  \"But, what of your lover...,\" I proffered.\n",
      "  \n",
      "  \"What?\"\n",
      "  \n",
      "  \"Your *LOVER*! I arranged those meetings. It was ME! You, an \n",
      "attorney,\" I sighed, and gushed on, \"I brought you two together. \n",
      "I responded to your personal ad. Yes, it was ME, who sent all those \n",
      "love letters you answered. There never was a woman. I dressed in drag \n",
      "to meet -- you. I'm your inamorato,\" I gushed imploringly.\n",
      "  \n",
      "  \"Darling! Do write again, but be brief,\" lawyer-like, he taunted,\n",
      "while holding me in his arms and nearer the edge, a sardonic smile \n",
      "etched his lips.\n",
      "  \n",
      "  I thought, \"_He's smiling. He wants me. We'll live happily ever \n",
      "after, no children, but no dirty diapers; more time for us._\" \n",
      "        \n",
      "  The situation got worse.  \n",
      "  \n",
      "  I went over the edge -- literally!        \n",
      "\n",
      "Copyright 1993 Francis U. Kaltenbaugh\n",
      "-------------------------     # # #    ----------------------------------\n",
      "Francis is one of those kinds of authors. I'm still trying to figure his/\n",
      "her political persuasions. One never knows does one. Writing for escapisim \n",
      "is a way of life, and sharing is a reward in itself, reports Francis.             \n",
      "==========================================================================\n"
     ]
    }
   ],
   "source": [
    "print_doc(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
